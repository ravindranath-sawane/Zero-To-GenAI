{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d29dc66",
   "metadata": {},
   "source": [
    "## 1. What is Generative AI?\n",
    "\n",
    "To understand Generative AI, let's first compare it with traditional AI approaches.\n",
    "\n",
    "### üîç Discriminative AI (Traditional AI)\n",
    "\n",
    "**Discriminative AI** is like a **judge at a dog show**. It looks at something and decides which category it belongs to.\n",
    "\n",
    "- **Task**: Classification, prediction, detection\n",
    "- **Example**: \"Is this a cat or a dog?\" ‚Üí \"It's a cat!\"\n",
    "- **How it works**: Learns the *boundaries* between different categories\n",
    "\n",
    "Think of it like a spam filter that looks at an email and decides: **Spam** or **Not Spam**?\n",
    "\n",
    "### ‚ú® Generative AI (The New Wave)\n",
    "\n",
    "**Generative AI** is like an **artist**. Instead of just judging things, it *creates* new things that never existed before!\n",
    "\n",
    "- **Task**: Creation, generation, synthesis\n",
    "- **Example**: \"Create a picture of a cat wearing a space suit\" ‚Üí üê±üöÄ\n",
    "- **How it works**: Learns the *patterns* of data so well that it can create new, similar data\n",
    "\n",
    "### üéØ Key Difference\n",
    "\n",
    "| Aspect | Discriminative AI | Generative AI |\n",
    "|--------|------------------|---------------|\n",
    "| **Question** | \"What is this?\" | \"Create something like this\" |\n",
    "| **Output** | A label/category | New content |\n",
    "| **Analogy** | A judge | An artist |\n",
    "| **Examples** | Spam filters, image classifiers | ChatGPT, DALL-E, Midjourney |\n",
    "\n",
    "> üí° **Simple Analogy**: Discriminative AI is like a food critic who tastes dishes and rates them. Generative AI is like a chef who creates entirely new recipes!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc5bb02",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. How LLMs Work: Next Token Prediction\n",
    "\n",
    "### üì± Autocomplete on Steroids!\n",
    "\n",
    "Have you ever typed a text message and your phone suggested the next word? That's basically what Large Language Models (LLMs) do, but **much, much smarter**.\n",
    "\n",
    "### The Core Idea: Next Token Prediction\n",
    "\n",
    "LLMs are trained to predict **\"What word (or token) comes next?\"**\n",
    "\n",
    "```\n",
    "Input: \"The cat sat on the\"\n",
    "LLM predicts: \"mat\" (or \"floor\", \"chair\", \"roof\", etc.)\n",
    "```\n",
    "\n",
    "### üß† But How Is This So Powerful?\n",
    "\n",
    "Here's the magic: When you train a model to predict the next word on **trillions of words** from books, websites, and articles, it has to learn:\n",
    "\n",
    "1. **Grammar** - to make sentences flow correctly\n",
    "2. **Facts** - \"Paris is the capital of...\" ‚Üí \"France\"\n",
    "3. **Reasoning** - to connect ideas logically\n",
    "4. **Context** - to understand what the conversation is about\n",
    "\n",
    "### üîÑ The Generation Loop\n",
    "\n",
    "When you ask ChatGPT a question, here's what happens:\n",
    "\n",
    "```\n",
    "You: \"What is the capital of France?\"\n",
    "\n",
    "LLM thinks: \"What is the capital of France?\" ‚Üí predicts \"The\"\n",
    "LLM thinks: \"What is the capital of France? The\" ‚Üí predicts \"capital\"\n",
    "LLM thinks: \"What is the capital of France? The capital\" ‚Üí predicts \"of\"\n",
    "... and so on until it predicts a stop signal\n",
    "\n",
    "Final output: \"The capital of France is Paris.\"\n",
    "```\n",
    "\n",
    "> üí° **Key Insight**: LLMs don't \"know\" things the way humans do. They're incredibly sophisticated pattern matchers that have seen so much text that they can generate coherent, useful responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afe893b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Key Terminology\n",
    "\n",
    "Let's learn the essential vocabulary you'll encounter when working with LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd48eff",
   "metadata": {},
   "source": [
    "### üß© 3.1 Tokens\n",
    "\n",
    "**Tokens are NOT the same as words!**\n",
    "\n",
    "A token is a chunk of text that the model processes. It could be:\n",
    "- A whole word: `hello` ‚Üí 1 token\n",
    "- Part of a word: `uncomfortable` ‚Üí `un` + `comfort` + `able` = 3 tokens\n",
    "- Punctuation: `!` ‚Üí 1 token\n",
    "- A space + word combination\n",
    "\n",
    "### Why Do Tokens Matter?\n",
    "\n",
    "1. **Cost**: API pricing is often based on tokens (e.g., $0.01 per 1,000 tokens)\n",
    "2. **Limits**: Models have maximum token limits (context window)\n",
    "3. **Speed**: More tokens = slower processing\n",
    "\n",
    "### üìè Rule of Thumb\n",
    "- **English**: ~1 token ‚âà 4 characters or ~0.75 words\n",
    "- **100 tokens** ‚âà 75 words\n",
    "\n",
    "Let's see this in action with some Python code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fc2672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Token Estimation\n",
    "# Rule of thumb: ~4 characters per token for English text\n",
    "\n",
    "def estimate_tokens(text):\n",
    "    \"\"\"Estimate the number of tokens in a text (rough approximation)\"\"\"\n",
    "    # Method 1: Character-based estimate (~4 chars per token)\n",
    "    char_estimate = len(text) / 4\n",
    "    \n",
    "    # Method 2: Word-based estimate (~0.75 words per token, or ~1.33 tokens per word)\n",
    "    words = text.split()\n",
    "    word_estimate = len(words) * 1.33\n",
    "    \n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"characters\": len(text),\n",
    "        \"words\": len(words),\n",
    "        \"estimated_tokens (char method)\": round(char_estimate),\n",
    "        \"estimated_tokens (word method)\": round(word_estimate)\n",
    "    }\n",
    "\n",
    "# Let's test with different sentences\n",
    "sentences = [\n",
    "    \"Hello, world!\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Artificial Intelligence is transforming the world.\",\n",
    "    \"supercalifragilisticexpialidocious\"  # One long word!\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    result = estimate_tokens(sentence)\n",
    "    print(f\"Text: '{result['text']}'\")\n",
    "    print(f\"  Characters: {result['characters']}, Words: {result['words']}\")\n",
    "    print(f\"  Estimated tokens: ~{result['estimated_tokens (word method)']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb88749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Using tiktoken for accurate token counting (OpenAI's tokenizer)\n",
    "# Uncomment and run if you have tiktoken installed: pip install tiktoken\n",
    "\n",
    "try:\n",
    "    import tiktoken\n",
    "    \n",
    "    # Get the tokenizer for GPT-4 / GPT-3.5\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    \n",
    "    sentences = [\n",
    "        \"Hello, world!\",\n",
    "        \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"Artificial Intelligence is transforming the world.\",\n",
    "        \"supercalifragilisticexpialidocious\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Accurate Token Counts using tiktoken:\\n\")\n",
    "    for sentence in sentences:\n",
    "        tokens = encoding.encode(sentence)\n",
    "        print(f\"Text: '{sentence}'\")\n",
    "        print(f\"  Actual tokens: {len(tokens)}\")\n",
    "        print(f\"  Token breakdown: {tokens}\")\n",
    "        print()\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"tiktoken not installed. Run: pip install tiktoken\")\n",
    "    print(\"For now, use the estimation method above!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afadc217",
   "metadata": {},
   "source": [
    "### üå°Ô∏è 3.2 Temperature\n",
    "\n",
    "**Temperature controls how \"creative\" or \"random\" the model's responses are.**\n",
    "\n",
    "Think of it like a dial that goes from **focused** to **creative**:\n",
    "\n",
    "```\n",
    "Temperature: 0 ‚Üê‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Üí 1\n",
    "             Focused           Creative\n",
    "             Deterministic     Random\n",
    "             Repetitive        Varied\n",
    "```\n",
    "\n",
    "### üéØ Temperature = 0 (Focused)\n",
    "- Always picks the **most likely** next token\n",
    "- Same input ‚Üí Same output (deterministic)\n",
    "- Best for: Facts, code, math, structured data\n",
    "\n",
    "### üé® Temperature = 1 (Creative)\n",
    "- Picks from a **wider range** of possible tokens\n",
    "- Same input ‚Üí Different outputs each time\n",
    "- Best for: Creative writing, brainstorming, poetry\n",
    "\n",
    "### üî• Temperature > 1 (Chaotic)\n",
    "- Very random, often nonsensical\n",
    "- Rarely used in practice\n",
    "\n",
    "### üìä Visual Analogy\n",
    "\n",
    "Imagine the model is choosing the next word after \"The cat sat on the...\":\n",
    "\n",
    "| Word | Probability |\n",
    "|------|-------------|\n",
    "| mat | 40% |\n",
    "| floor | 25% |\n",
    "| chair | 15% |\n",
    "| roof | 10% |\n",
    "| moon | 5% |\n",
    "| pizza | 5% |\n",
    "\n",
    "- **Temperature 0**: Always picks \"mat\" (highest probability)\n",
    "- **Temperature 0.7**: Usually \"mat\" or \"floor\", sometimes \"chair\"\n",
    "- **Temperature 1**: Might even pick \"moon\" or \"pizza\" occasionally!\n",
    "\n",
    "> üí° **Pro Tip**: Start with temperature 0.7 for most tasks. Lower it for factual tasks, raise it for creative ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41e06ce",
   "metadata": {},
   "source": [
    "### üìö 3.3 Context Window\n",
    "\n",
    "**The context window is the model's \"short-term memory\" ‚Äî how much text it can \"see\" at once.**\n",
    "\n",
    "### üß† What Is It?\n",
    "\n",
    "When you chat with an LLM, it doesn't actually \"remember\" your conversation. Instead, the **entire conversation** is sent to the model each time, and it has a limit on how much it can process.\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ         CONTEXT WINDOW              ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ\n",
    "‚îÇ  ‚îÇ System prompt               ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ User message 1              ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ Assistant response 1        ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ User message 2              ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ Assistant response 2        ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îÇ User message 3              ‚îÇ ‚Üê‚îÄ‚îÄ‚îº‚îÄ‚îÄ Current conversation\n",
    "‚îÇ  ‚îÇ ...                         ‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         Maximum: X tokens\n",
    "```\n",
    "\n",
    "### üìè Context Window Sizes (Examples)\n",
    "\n",
    "| Model | Context Window | Approximate Pages |\n",
    "|-------|---------------|-------------------|\n",
    "| GPT-3.5 | 4,096 tokens | ~6 pages |\n",
    "| GPT-4 | 8,192 tokens | ~12 pages |\n",
    "| GPT-4 Turbo | 128,000 tokens | ~200 pages |\n",
    "| Claude 3 | 200,000 tokens | ~300 pages |\n",
    "\n",
    "### ‚ö†Ô∏è What Happens When You Exceed It?\n",
    "\n",
    "1. **Old messages get \"forgotten\"** (dropped from the beginning)\n",
    "2. **The model loses context** about earlier parts of the conversation\n",
    "3. **You might get inconsistent responses**\n",
    "\n",
    "### üí° Practical Tips\n",
    "\n",
    "1. **Keep conversations focused** - Don't include unnecessary information\n",
    "2. **Summarize long documents** - Instead of pasting a whole book\n",
    "3. **Start fresh** - When the model seems confused, start a new conversation\n",
    "4. **Use RAG** - For working with large documents (we'll cover this later!)\n",
    "\n",
    "> üéØ **Analogy**: Think of the context window like a desk. You can only have so many papers on your desk at once. If you want to add more, some papers have to go!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6958665c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Summary üìù\n",
    "\n",
    "Congratulations! You've learned the fundamentals of Generative AI. Here's a quick recap:\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Generative AI vs Discriminative AI**\n",
    "  - Discriminative AI = Judge (classifies/categorizes)\n",
    "  - Generative AI = Artist (creates new content)\n",
    "\n",
    "- **How LLMs Work**\n",
    "  - LLMs predict the next token (like autocomplete on steroids)\n",
    "  - They generate text one token at a time in a loop\n",
    "  - Training on massive text data teaches them language, facts, and reasoning\n",
    "\n",
    "- **Tokens**\n",
    "  - Tokens ‚â† Words (usually 1 token ‚âà 4 characters)\n",
    "  - Important for cost, speed, and limits\n",
    "  - Use `tiktoken` for accurate counting with OpenAI models\n",
    "\n",
    "- **Temperature**\n",
    "  - Controls randomness/creativity (0 = focused, 1 = creative)\n",
    "  - Low temperature for facts, high for creativity\n",
    "  - Default recommendation: 0.7\n",
    "\n",
    "- **Context Window**\n",
    "  - The model's \"short-term memory\"\n",
    "  - Limited size (varies by model)\n",
    "  - Entire conversation must fit within it\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ What's Next?\n",
    "\n",
    "In the upcoming notebooks, we'll explore:\n",
    "- **Prompt Engineering** - How to communicate effectively with LLMs\n",
    "- **Working with APIs** - Building applications with GPT and other models\n",
    "- **RAG (Retrieval Augmented Generation)** - Giving LLMs access to your own data\n",
    "\n",
    "Happy learning! üéâ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
