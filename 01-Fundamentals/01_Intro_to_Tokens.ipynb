{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "613b5d82",
   "metadata": {},
   "source": [
    "## ğŸ“š Quick Recap: Generative vs Discriminative AI\n",
    "\n",
    "Before we dive into tokens, let's quickly understand the two main types of AI:\n",
    "\n",
    "### ğŸ” Discriminative AI\n",
    "**\"What category does this belong to?\"**\n",
    "\n",
    "- Acts like a **judge** or **classifier**\n",
    "- Examples: Spam detection, image classification, fraud detection\n",
    "- Input: Data â†’ Output: Label/Category\n",
    "\n",
    "### âœ¨ Generative AI\n",
    "**\"Create something new that looks like the training data\"**\n",
    "\n",
    "- Acts like an **artist** or **creator**\n",
    "- Examples: ChatGPT, DALL-E, Midjourney, Claude\n",
    "- Input: Prompt â†’ Output: New content (text, images, code)\n",
    "\n",
    "> ğŸ’¡ **The Magic**: Generative AI learns patterns so well that it can create entirely new content that never existed before!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342ec798",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“± How LLMs Work: Next Token Prediction\n",
    "\n",
    "### The Smartphone Autocomplete Analogy\n",
    "\n",
    "You know how your phone suggests the next word when you're typing a message?\n",
    "\n",
    "```\n",
    "You type: \"I'll meet you at the\"\n",
    "Phone suggests: [coffee] [park] [office]\n",
    "```\n",
    "\n",
    "**Large Language Models (LLMs) work exactly like this â€” but on steroids!**\n",
    "\n",
    "### The Core Mechanism\n",
    "\n",
    "1. LLMs are trained on **trillions of words** from the internet\n",
    "2. They learn to predict: **\"What comes next?\"**\n",
    "3. Generation happens **one token at a time** in a loop\n",
    "\n",
    "```\n",
    "Prompt: \"The capital of France is\"\n",
    "\n",
    "Step 1: Predict next token â†’ \"Paris\"\n",
    "Step 2: Predict next token â†’ \".\"\n",
    "Step 3: Predict next token â†’ [STOP]\n",
    "\n",
    "Output: \"Paris.\"\n",
    "```\n",
    "\n",
    "### Why Is This So Powerful?\n",
    "\n",
    "To predict the next word accurately, the model **must learn**:\n",
    "- âœ… Grammar and syntax\n",
    "- âœ… Facts and knowledge\n",
    "- âœ… Reasoning and logic\n",
    "- âœ… Context and relationships\n",
    "\n",
    "> ğŸ¯ **Key Insight**: The simple task of \"predict the next token\" forces the model to learn an incredible amount about language and the world!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d792a456",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ§© What Are Tokens?\n",
    "\n",
    "### Tokens â‰  Words!\n",
    "\n",
    "This is one of the most important concepts to understand:\n",
    "\n",
    "**A token is a chunk of text that the model processes.** It could be:\n",
    "- A whole word: `hello` â†’ 1 token\n",
    "- Part of a word: `hamburger` â†’ `ham` + `burger` = 2 tokens\n",
    "- A single character: `!` â†’ 1 token\n",
    "- A space: ` ` â†’ often combined with the next word\n",
    "\n",
    "### Why Tokens Instead of Words?\n",
    "\n",
    "1. **Efficiency**: Common words = 1 token, rare words = multiple tokens\n",
    "2. **Flexibility**: Can handle any language, code, or special characters\n",
    "3. **Vocabulary Size**: Limited vocabulary (~50,000-100,000 tokens) can represent infinite text\n",
    "\n",
    "### Why Do Tokens Matter?\n",
    "\n",
    "| Reason | Impact |\n",
    "|--------|--------|\n",
    "| **Cost** | API pricing is per token ($X per 1M tokens) |\n",
    "| **Speed** | More tokens = slower generation |\n",
    "| **Limits** | Context window measured in tokens |\n",
    "| **Quality** | Understanding tokenization helps write better prompts |\n",
    "\n",
    "Let's see this in action! ğŸ‘‡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b685430",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ’» Hands-On: Tokenization with `tiktoken`\n",
    "\n",
    "Let's use OpenAI's `tiktoken` library to see exactly how text gets converted to tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60099c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's install tiktoken if needed\n",
    "# !pip install tiktoken\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "# Get the tokenizer for GPT-4 / GPT-4o / GPT-3.5\n",
    "# \"cl100k_base\" is the encoding used by these models\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "\n",
    "print(\"âœ… Tokenizer loaded successfully!\")\n",
    "print(f\"ğŸ“Š Vocabulary size: {encoding.n_vocab:,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876299ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text: str, show_tokens: bool = True) -> int:\n",
    "    \"\"\"\n",
    "    Count the number of tokens in a text string.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to tokenize\n",
    "        show_tokens: Whether to display the individual tokens\n",
    "    \n",
    "    Returns:\n",
    "        The number of tokens\n",
    "    \"\"\"\n",
    "    # Encode text to token IDs (integers)\n",
    "    token_ids = encoding.encode(text)\n",
    "    \n",
    "    # Decode each token ID back to text to see what each token represents\n",
    "    tokens = [encoding.decode([tid]) for tid in token_ids]\n",
    "    \n",
    "    print(f\"ğŸ“ Text: '{text}'\")\n",
    "    print(f\"ğŸ”¢ Token count: {len(token_ids)}\")\n",
    "    \n",
    "    if show_tokens:\n",
    "        print(f\"ğŸ§© Token IDs: {token_ids}\")\n",
    "        print(f\"ğŸ“¦ Tokens: {tokens}\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    return len(token_ids)\n",
    "\n",
    "print(\"âœ… Helper function created! Let's use it below.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5153f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ Example 1: Simple words\n",
    "print(\"=\" * 50)\n",
    "print(\"EXAMPLE 1: Simple Words\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "count_tokens(\"Apple\")        # Common word = 1 token\n",
    "count_tokens(\"Hamburger\")    # Compound word = might be split!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae9fc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”¤ Example 2: Common vs Rare Words\n",
    "print(\"=\" * 50)\n",
    "print(\"EXAMPLE 2: Common vs Rare Words\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "count_tokens(\"the\")                        # Super common = 1 token\n",
    "count_tokens(\"hello\")                      # Common = 1 token  \n",
    "count_tokens(\"cryptocurrency\")             # Technical term\n",
    "count_tokens(\"supercalifragilisticexpialidocious\")  # Made-up long word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92900be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š Example 3: Sentences\n",
    "print(\"=\" * 50)\n",
    "print(\"EXAMPLE 3: Full Sentences\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "count_tokens(\"Hello, world!\")\n",
    "count_tokens(\"The quick brown fox jumps over the lazy dog.\")\n",
    "count_tokens(\"Artificial Intelligence is transforming the world.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b698253e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ® YOUR TURN: Try your own text!\n",
    "print(\"=\" * 50)\n",
    "print(\"ğŸ® YOUR TURN!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Change this text to anything you want to explore!\n",
    "your_text = \"I am learning about Generative AI and tokens!\"\n",
    "\n",
    "count_tokens(your_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34066519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“ Token Estimation Rule of Thumb\n",
    "print(\"=\" * 50)\n",
    "print(\"ğŸ“ RULE OF THUMB VALIDATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_text = \"\"\"Large Language Models (LLMs) are a type of artificial intelligence \n",
    "that can understand and generate human-like text. They are trained on vast amounts \n",
    "of text data from the internet, books, and other sources.\"\"\"\n",
    "\n",
    "words = len(test_text.split())\n",
    "characters = len(test_text)\n",
    "actual_tokens = count_tokens(test_text, show_tokens=False)\n",
    "\n",
    "print(f\"\\nğŸ“Š Comparison:\")\n",
    "print(f\"   Words: {words}\")\n",
    "print(f\"   Characters: {characters}\")\n",
    "print(f\"   Actual Tokens: {actual_tokens}\")\n",
    "print(f\"   Ratio (tokens/words): {actual_tokens/words:.2f}\")\n",
    "print(f\"   Ratio (chars/tokens): {characters/actual_tokens:.2f}\")\n",
    "print(f\"\\nğŸ’¡ Rule of thumb: ~1.3 tokens per word, ~4 chars per token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe87790",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸŒ¡ï¸ Temperature: Controlling AI Creativity\n",
    "\n",
    "When an LLM predicts the next token, it actually calculates **probabilities** for all possible tokens. **Temperature** controls how we pick from these probabilities.\n",
    "\n",
    "### The Analogy: Two Personalities\n",
    "\n",
    "#### ğŸ§® Temperature = 0.0 â†’ The Strict Accountant\n",
    "- Always picks the **most likely** (highest probability) token\n",
    "- Deterministic: Same input â†’ Same output every time\n",
    "- Best for: Facts, math, code, structured data\n",
    "- Personality: Precise, predictable, no surprises\n",
    "\n",
    "#### ğŸ¨ Temperature = 1.0 â†’ The Creative Poet\n",
    "- Picks from a **wider distribution** of possible tokens\n",
    "- Stochastic: Same input â†’ Different outputs each time\n",
    "- Best for: Creative writing, brainstorming, storytelling\n",
    "- Personality: Imaginative, varied, sometimes surprising\n",
    "\n",
    "### Visual Example\n",
    "\n",
    "Prompt: `\"The sunset was...\"`\n",
    "\n",
    "| Token | Probability |\n",
    "|-------|-------------|\n",
    "| beautiful | 35% |\n",
    "| stunning | 25% |\n",
    "| amazing | 15% |\n",
    "| breathtaking | 10% |\n",
    "| purple | 5% |\n",
    "| melancholic | 5% |\n",
    "| existential | 3% |\n",
    "| dancing | 2% |\n",
    "\n",
    "- **Accountant (T=0)**: Always says \"beautiful\" (highest probability)\n",
    "- **Poet (T=1)**: Might say \"melancholic\" or even \"dancing\" â€” more creative!\n",
    "\n",
    "### ğŸ“Š Temperature Scale\n",
    "\n",
    "```\n",
    "Temperature:  0.0 â†â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â†’ 1.0 â€•â€•â†’ 2.0\n",
    "              |                        |        |\n",
    "         Accountant               Balanced   Chaotic\n",
    "         (Focused)                 (0.7)    (Nonsense)\n",
    "```\n",
    "\n",
    "### ğŸ’¡ Practical Guidelines\n",
    "\n",
    "| Use Case | Recommended Temperature |\n",
    "|----------|------------------------|\n",
    "| Code generation | 0.0 - 0.2 |\n",
    "| Factual Q&A | 0.0 - 0.3 |\n",
    "| General tasks | 0.5 - 0.7 |\n",
    "| Creative writing | 0.7 - 0.9 |\n",
    "| Brainstorming | 0.8 - 1.0 |\n",
    "\n",
    "> ğŸ¯ **Pro Tip**: Start with **0.7** for most tasks. Lower it if you need consistency, raise it if you want variety!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179228c9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ Summary\n",
    "\n",
    "### What You Learned Today\n",
    "\n",
    "âœ… **Generative vs Discriminative AI**\n",
    "- Discriminative = Judge (classifies)\n",
    "- Generative = Artist (creates)\n",
    "\n",
    "âœ… **Next Token Prediction**\n",
    "- LLMs work like smartphone autocomplete, but much smarter\n",
    "- They generate text one token at a time\n",
    "\n",
    "âœ… **Tokens**\n",
    "- Tokens â‰  Words\n",
    "- ~1.3 tokens per word, ~4 characters per token\n",
    "- Important for cost, speed, and context limits\n",
    "- Use `tiktoken` to count tokens accurately\n",
    "\n",
    "âœ… **Temperature**\n",
    "- 0.0 = Strict Accountant (focused, deterministic)\n",
    "- 1.0 = Creative Poet (varied, imaginative)\n",
    "- Default: 0.7 for balanced results\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸš€ Next Steps\n",
    "\n",
    "In the next module, we'll explore **Prompt Engineering** â€” the art of communicating effectively with LLMs to get the best results!\n",
    "\n",
    "Happy learning! ğŸ‰"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
