{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee2f0f9f",
   "metadata": {},
   "source": [
    "## 1. üîß Setup\n",
    "\n",
    "First, let's load our environment and set up LangChain with modern 2025 imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4d69a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Modern LangChain imports (2025 standard)\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.chat_history import BaseChatMessageHistory, InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Load API key from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API key is loaded\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"‚úÖ OPENAI_API_KEY loaded successfully!\")\n",
    "else:\n",
    "    print(\"‚ùå ERROR: OPENAI_API_KEY not found. Please check your .env file.\")\n",
    "\n",
    "# Initialize the model\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "print(f\"ü§ñ Model initialized: {llm.model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81f2fde",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. üî¥ The Problem: LLMs Have No Memory\n",
    "\n",
    "Let's first demonstrate that LLMs are **stateless** ‚Äî they don't remember previous messages by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6d181f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# DEMONSTRATION: No Memory Without History\n",
    "# ===========================================\n",
    "\n",
    "print(\"üî¥ WITHOUT MEMORY MANAGEMENT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# First message\n",
    "response1 = llm.invoke([HumanMessage(content=\"My name is Alice.\")])\n",
    "print(f\"üë§ User: My name is Alice.\")\n",
    "print(f\"ü§ñ AI: {response1.content}\")\n",
    "print()\n",
    "\n",
    "# Second message - SEPARATE call, no history!\n",
    "response2 = llm.invoke([HumanMessage(content=\"What is my name?\")])\n",
    "print(f\"üë§ User: What is my name?\")\n",
    "print(f\"ü§ñ AI: {response2.content}\")\n",
    "print()\n",
    "print(\"‚ùå The AI doesn't know our name because each call is independent!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63c0c91",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. üü¢ The Solution: LangChain Memory\n",
    "\n",
    "LangChain provides `InMemoryChatMessageHistory` and `RunnableWithMessageHistory` to automatically manage conversation history.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. **ChatMessageHistory**: Stores the conversation messages\n",
    "2. **RunnableWithMessageHistory**: Wraps your chain and automatically:\n",
    "   - Loads previous messages before each call\n",
    "   - Saves new messages after each call\n",
    "3. **Session IDs**: Allow multiple separate conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b96061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# SETUP: Memory-Enabled Chain\n",
    "# ===========================================\n",
    "\n",
    "# Step 1: Create a prompt template with a placeholder for chat history\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"You are a helpful, friendly AI assistant. Remember details the user tells you.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),  # <-- This is where history goes!\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Step 2: Create the chain (prompt | model)\n",
    "chain = prompt | llm\n",
    "\n",
    "# Step 3: Create a store for session histories\n",
    "# Each session_id gets its own separate conversation history\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    \"\"\"\n",
    "    Get or create a chat history for a given session.\n",
    "    \n",
    "    This function is called by RunnableWithMessageHistory to:\n",
    "    - Load existing history before a call\n",
    "    - Save new messages after a call\n",
    "    \"\"\"\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# Step 4: Wrap the chain with memory management\n",
    "chain_with_memory = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Memory-enabled chain created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc04330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# DEMONSTRATION: Memory in Action!\n",
    "# ===========================================\n",
    "\n",
    "print(\"üü¢ WITH LANGCHAIN MEMORY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Configuration for this session\n",
    "config = {\"configurable\": {\"session_id\": \"alice_session\"}}\n",
    "\n",
    "# First message: Tell the AI our name\n",
    "response1 = chain_with_memory.invoke(\n",
    "    {\"input\": \"My name is Alice.\"},\n",
    "    config=config\n",
    ")\n",
    "print(f\"üë§ User: My name is Alice.\")\n",
    "print(f\"ü§ñ AI: {response1.content}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5b275d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second message: Ask the AI our name\n",
    "# The AI should remember because of the memory management!\n",
    "\n",
    "response2 = chain_with_memory.invoke(\n",
    "    {\"input\": \"What is my name?\"},\n",
    "    config=config  # Same session_id = same conversation\n",
    ")\n",
    "print(f\"üë§ User: What is my name?\")\n",
    "print(f\"ü§ñ AI: {response2.content}\")\n",
    "print()\n",
    "print(\"‚úÖ The AI remembers our name because LangChain manages the history!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767b3353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's continue the conversation with more context\n",
    "\n",
    "response3 = chain_with_memory.invoke(\n",
    "    {\"input\": \"I'm learning about AI. I work as a software engineer.\"},\n",
    "    config=config\n",
    ")\n",
    "print(f\"üë§ User: I'm learning about AI. I work as a software engineer.\")\n",
    "print(f\"ü§ñ AI: {response3.content}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e81a799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if it remembers BOTH pieces of information\n",
    "\n",
    "response4 = chain_with_memory.invoke(\n",
    "    {\"input\": \"What do you know about me so far?\"},\n",
    "    config=config\n",
    ")\n",
    "print(f\"üë§ User: What do you know about me so far?\")\n",
    "print(f\"ü§ñ AI: {response4.content}\")\n",
    "print()\n",
    "print(\"üéâ The AI remembers everything from the conversation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efca14dd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. üìú Inspecting the Stored History\n",
    "\n",
    "Let's peek inside the memory store to see what LangChain is actually storing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a92f77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# INSPECT: What's in the memory store?\n",
    "# ===========================================\n",
    "\n",
    "print(\"üìú CONVERSATION HISTORY STORED BY LANGCHAIN\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get the history for our session\n",
    "history = get_session_history(\"alice_session\")\n",
    "\n",
    "# Print each message\n",
    "for i, message in enumerate(history.messages, 1):\n",
    "    role = type(message).__name__.replace(\"Message\", \"\")\n",
    "    content = message.content[:80] + \"...\" if len(message.content) > 80 else message.content\n",
    "    print(f\"{i}. [{role}]: {content}\")\n",
    "\n",
    "print()\n",
    "print(f\"üìä Total messages stored: {len(history.messages)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa17d3a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. üîÄ Multiple Sessions (Separate Conversations)\n",
    "\n",
    "The `session_id` allows you to have multiple independent conversations. This is useful for:\n",
    "- Multi-user applications\n",
    "- Separate conversation threads\n",
    "- Testing different scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393f8cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# MULTIPLE SESSIONS DEMO\n",
    "# ===========================================\n",
    "\n",
    "print(\"üîÄ MULTIPLE INDEPENDENT SESSIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Session 1: Bob's conversation\n",
    "bob_config = {\"configurable\": {\"session_id\": \"bob_session\"}}\n",
    "\n",
    "response_bob = chain_with_memory.invoke(\n",
    "    {\"input\": \"Hi! My name is Bob and I love pizza.\"},\n",
    "    config=bob_config\n",
    ")\n",
    "print(f\"üîµ Bob's Session:\")\n",
    "print(f\"   üë§ Bob: Hi! My name is Bob and I love pizza.\")\n",
    "print(f\"   ü§ñ AI: {response_bob.content[:100]}...\")\n",
    "print()\n",
    "\n",
    "# Session 2: Charlie's conversation (separate!)\n",
    "charlie_config = {\"configurable\": {\"session_id\": \"charlie_session\"}}\n",
    "\n",
    "response_charlie = chain_with_memory.invoke(\n",
    "    {\"input\": \"Hello! I'm Charlie and I'm a musician.\"},\n",
    "    config=charlie_config\n",
    ")\n",
    "print(f\"üü° Charlie's Session:\")\n",
    "print(f\"   üë§ Charlie: Hello! I'm Charlie and I'm a musician.\")\n",
    "print(f\"   ü§ñ AI: {response_charlie.content[:100]}...\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7406eb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify sessions are independent\n",
    "\n",
    "print(\"üîç VERIFYING SESSION INDEPENDENCE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Ask Bob's session about the user\n",
    "bob_check = chain_with_memory.invoke(\n",
    "    {\"input\": \"What's my name and what do I like?\"},\n",
    "    config=bob_config\n",
    ")\n",
    "print(f\"üîµ Bob's Session - 'What's my name and what do I like?'\")\n",
    "print(f\"   ü§ñ AI: {bob_check.content}\")\n",
    "print()\n",
    "\n",
    "# Ask Charlie's session about the user\n",
    "charlie_check = chain_with_memory.invoke(\n",
    "    {\"input\": \"What's my name and what do I do?\"},\n",
    "    config=charlie_config\n",
    ")\n",
    "print(f\"üü° Charlie's Session - 'What's my name and what do I do?'\")\n",
    "print(f\"   ü§ñ AI: {charlie_check.content}\")\n",
    "print()\n",
    "\n",
    "print(\"‚úÖ Each session remembers only its own conversation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9ef781",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Summary\n",
    "\n",
    "### What You Learned Today\n",
    "\n",
    "‚úÖ **The Problem**\n",
    "- LLMs are stateless ‚Äî they don't remember previous messages\n",
    "- Without memory management, each API call is independent\n",
    "\n",
    "‚úÖ **Manual Solution** (from `simple_chatbot.py`)\n",
    "- Maintain a `conversation_history` list\n",
    "- Append every message (user + AI) to the list\n",
    "- Send the entire history with each request\n",
    "\n",
    "‚úÖ **LangChain Solution** (this notebook)\n",
    "- `InMemoryChatMessageHistory`: Stores messages in memory\n",
    "- `RunnableWithMessageHistory`: Automatically loads/saves history\n",
    "- `session_id`: Enables multiple independent conversations\n",
    "\n",
    "### Key Components\n",
    "\n",
    "```python\n",
    "# 1. Create history store\n",
    "store = {}\n",
    "def get_session_history(session_id):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# 2. Wrap chain with memory\n",
    "chain_with_memory = RunnableWithMessageHistory(\n",
    "    chain, get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "\n",
    "# 3. Use with session config\n",
    "config = {\"configurable\": {\"session_id\": \"my_session\"}}\n",
    "response = chain_with_memory.invoke({\"input\": \"Hello!\"}, config=config)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Next Steps\n",
    "\n",
    "In the next module, we'll explore **RAG (Retrieval Augmented Generation)** ‚Äî giving your AI access to external knowledge!\n",
    "\n",
    "Happy building! üéâ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
