{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87a21e73",
   "metadata": {},
   "source": [
    "## ğŸ§  Key Concept: What Are Embeddings?\n",
    "\n",
    "Before we dive in, let's understand the magic behind RAG: **Embeddings**.\n",
    "\n",
    "### The Problem\n",
    "Computers don't understand text. They only understand numbers.\n",
    "\n",
    "### The Solution: Embeddings\n",
    "**Embeddings convert text into a list of numbers (a vector)** that captures the *meaning* of the text.\n",
    "\n",
    "```\n",
    "\"I love pizza\" â†’ [0.12, -0.45, 0.89, 0.33, ..., 0.21]  (1536 numbers for OpenAI)\n",
    "\"Pizza is great\" â†’ [0.11, -0.44, 0.87, 0.35, ..., 0.19]  (very similar!)\n",
    "\"I hate Mondays\" â†’ [-0.55, 0.22, 0.11, -0.67, ..., 0.45]  (very different!)\n",
    "```\n",
    "\n",
    "### Why This Matters\n",
    "- Similar meanings â†’ Similar vectors â†’ Close in \"vector space\"\n",
    "- We can find relevant documents by finding vectors closest to the question's vector\n",
    "- This is called **semantic search** (search by meaning, not just keywords)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611514fb",
   "metadata": {},
   "source": [
    "## ğŸ”§ Setup\n",
    "\n",
    "Let's load our environment and import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b76a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Modern LangChain imports (2025 standard - split packages)\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Load API key from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API key\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"âœ… OPENAI_API_KEY loaded successfully!\")\n",
    "else:\n",
    "    print(\"âŒ ERROR: OPENAI_API_KEY not found. Please check your .env file.\")\n",
    "\n",
    "print(\"ğŸ“¦ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dbbf7d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: ğŸ“„ Load Data\n",
    "\n",
    "First, we need to get our documents into the system. LangChain provides **Document Loaders** for various file types:\n",
    "- `TextLoader` â†’ Plain text files\n",
    "- `PyPDFLoader` â†’ PDF files\n",
    "- `CSVLoader` â†’ CSV files\n",
    "- And many more!\n",
    "\n",
    "For this example, we'll use a simple text file with some \"secret\" information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcc0fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# STEP 1: LOAD DATA\n",
    "# ===========================================\n",
    "\n",
    "# Load our sample text file\n",
    "# This file contains information that the base LLM doesn't know!\n",
    "loader = TextLoader(\"sample.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "print(\"ğŸ“„ LOADED DOCUMENTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Number of documents: {len(documents)}\")\n",
    "print(f\"\\nDocument content:\")\n",
    "print(\"-\" * 50)\n",
    "print(documents[0].page_content)\n",
    "print(\"-\" * 50)\n",
    "print(f\"\\nMetadata: {documents[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490e6ca9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: âœ‚ï¸ Split Data into Chunks\n",
    "\n",
    "### Why Split?\n",
    "\n",
    "1. **Context Window Limits**: LLMs can only process so much text at once\n",
    "2. **Relevance**: Smaller chunks = more precise retrieval\n",
    "3. **Cost**: Sending entire documents is expensive\n",
    "\n",
    "### The RecursiveCharacterTextSplitter\n",
    "\n",
    "This splitter tries to keep related content together by:\n",
    "1. First trying to split on paragraphs (`\\n\\n`)\n",
    "2. Then sentences (`\\n`)\n",
    "3. Then spaces\n",
    "4. Finally, characters\n",
    "\n",
    "### Why Overlap Matters! ğŸ”‘\n",
    "\n",
    "```\n",
    "Without overlap:  [Chunk 1: \"The vault code is\"] [Chunk 2: \"9988. The manager\"]\n",
    "                   â†‘ This chunk misses the code!   â†‘ This chunk misses context!\n",
    "\n",
    "With overlap:     [Chunk 1: \"The vault code is 9988.\"] [Chunk 2: \"code is 9988. The manager\"]\n",
    "                   â†‘ Complete information!           â†‘ Overlapping context preserved!\n",
    "```\n",
    "\n",
    "**Overlap ensures that important information at chunk boundaries isn't lost!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2734417a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# STEP 2: SPLIT DATA INTO CHUNKS\n",
    "# ===========================================\n",
    "\n",
    "# Create a text splitter\n",
    "# - chunk_size: Maximum characters per chunk\n",
    "# - chunk_overlap: Characters to repeat between chunks (preserves context!)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,      # Small chunks for demonstration\n",
    "    chunk_overlap=20,    # 20 characters overlap between chunks\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Try these separators in order\n",
    ")\n",
    "\n",
    "# Split the documents\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(\"âœ‚ï¸ SPLIT INTO CHUNKS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Original documents: {len(documents)}\")\n",
    "print(f\"After splitting: {len(chunks)} chunks\")\n",
    "print(f\"\\nChunk size: 100 characters\")\n",
    "print(f\"Overlap: 20 characters\")\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ğŸ“¦ INDIVIDUAL CHUNKS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"\\nğŸ”¹ Chunk {i} ({len(chunk.page_content)} chars):\")\n",
    "    print(f\"   '{chunk.page_content}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43f1545",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: ğŸ”¢ Embed & Store\n",
    "\n",
    "Now we convert our text chunks into **embeddings** (vectors) and store them in a **vector database**.\n",
    "\n",
    "### What Happens Here:\n",
    "\n",
    "1. **OpenAIEmbeddings**: Calls OpenAI's API to convert each chunk into a 1536-dimensional vector\n",
    "2. **Chroma**: A lightweight vector database that stores these vectors and allows fast similarity search\n",
    "\n",
    "### The Magic\n",
    "```\n",
    "\"The vault code is 9988\" â†’ [0.12, -0.45, 0.89, ..., 0.21] (1536 numbers)\n",
    "\"What is the vault code?\" â†’ [0.11, -0.44, 0.88, ..., 0.20] (similar numbers!)\n",
    "```\n",
    "\n",
    "When we search, we find chunks with vectors most similar to our question's vector!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8412673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# STEP 3: EMBED & STORE\n",
    "# ===========================================\n",
    "\n",
    "# Initialize the embedding model\n",
    "# This converts text â†’ vectors (lists of numbers)\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\"  # Cost-effective embedding model\n",
    ")\n",
    "\n",
    "print(\"ğŸ”¢ CREATING EMBEDDINGS & VECTOR STORE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create the vector store from our chunks\n",
    "# This does several things:\n",
    "# 1. Sends each chunk to OpenAI to get its embedding\n",
    "# 2. Stores the chunks and their embeddings in Chroma\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"rag_demo\"  # Name for this collection\n",
    ")\n",
    "\n",
    "print(\"âœ… Vector store created!\")\n",
    "print(f\"ğŸ“Š Stored {len(chunks)} chunks with embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49fde4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what an embedding looks like!\n",
    "sample_text = \"What is the vault code?\"\n",
    "sample_embedding = embeddings.embed_query(sample_text)\n",
    "\n",
    "print(\"ğŸ” EMBEDDING EXAMPLE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Text: '{sample_text}'\")\n",
    "print(f\"\\nEmbedding dimensions: {len(sample_embedding)}\")\n",
    "print(f\"First 10 values: {sample_embedding[:10]}\")\n",
    "print(f\"\\nğŸ’¡ Each text becomes a {len(sample_embedding)}-dimensional vector!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068ec56e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: ğŸ¯ Retrieve Relevant Chunks\n",
    "\n",
    "Now let's test the retrieval! When we ask a question:\n",
    "1. The question is converted to an embedding\n",
    "2. We find the chunks with the most similar embeddings\n",
    "3. Those chunks are returned as \"context\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bb5cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# STEP 4: RETRIEVE RELEVANT CHUNKS\n",
    "# ===========================================\n",
    "\n",
    "# Create a retriever from the vector store\n",
    "# k=2 means: return the 2 most relevant chunks\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 2}\n",
    ")\n",
    "\n",
    "print(\"ğŸ¯ TESTING RETRIEVAL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test with a question\n",
    "test_question = \"What is the vault code?\"\n",
    "retrieved_docs = retriever.invoke(test_question)\n",
    "\n",
    "print(f\"Question: '{test_question}'\")\n",
    "print(f\"\\nRetrieved {len(retrieved_docs)} relevant chunks:\\n\")\n",
    "\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\"ğŸ“„ Chunk {i}:\")\n",
    "    print(f\"   '{doc.page_content}'\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4addcf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test another question\n",
    "test_question2 = \"Who is the manager?\"\n",
    "retrieved_docs2 = retriever.invoke(test_question2)\n",
    "\n",
    "print(f\"Question: '{test_question2}'\")\n",
    "print(f\"\\nRetrieved {len(retrieved_docs2)} relevant chunks:\\n\")\n",
    "\n",
    "for i, doc in enumerate(retrieved_docs2, 1):\n",
    "    print(f\"ğŸ“„ Chunk {i}:\")\n",
    "    print(f\"   '{doc.page_content}'\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da96b97b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: ğŸ¤– Generate Answer with RAG Chain\n",
    "\n",
    "Now we put it all together! We'll create a **RAG Chain** using LangChain Expression Language (LCEL).\n",
    "\n",
    "### The Chain Flow:\n",
    "\n",
    "```\n",
    "Question â†’ [Retriever finds context] â†’ [Prompt combines context + question] â†’ [LLM generates answer]\n",
    "```\n",
    "\n",
    "### LCEL Syntax:\n",
    "```python\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | output_parser\n",
    ")\n",
    "```\n",
    "\n",
    "This reads as: \"Take the input, use it to get context from retriever, pass the question through, then send to prompt, then to model, then parse the output.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d22ff09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# STEP 5: BUILD THE RAG CHAIN\n",
    "# ===========================================\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Create the prompt template\n",
    "# This tells the LLM how to use the retrieved context\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a helpful assistant. Answer the question based ONLY on the following context.\n",
    "If the answer is not in the context, say \"I don't have that information.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\")\n",
    "\n",
    "# Helper function to format retrieved documents\n",
    "def format_docs(docs):\n",
    "    \"\"\"Combine retrieved documents into a single string.\"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Build the RAG chain using LCEL (LangChain Expression Language)\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,  # Get docs and format them\n",
    "        \"question\": RunnablePassthrough()     # Pass the question through unchanged\n",
    "    }\n",
    "    | prompt      # Fill in the prompt template\n",
    "    | llm         # Send to the LLM\n",
    "    | StrOutputParser()  # Extract the string response\n",
    ")\n",
    "\n",
    "print(\"âœ… RAG Chain created!\")\n",
    "print(\"\\nğŸ“Š Chain components:\")\n",
    "print(\"   1. Retriever â†’ Finds relevant context\")\n",
    "print(\"   2. Prompt â†’ Combines context + question\")\n",
    "print(\"   3. LLM â†’ Generates answer\")\n",
    "print(\"   4. Parser â†’ Extracts text response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cce62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# TEST THE RAG CHAIN!\n",
    "# ===========================================\n",
    "\n",
    "print(\"ğŸš€ RAG CHAIN IN ACTION!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Question 1: The vault code\n",
    "question1 = \"What is the vault code?\"\n",
    "answer1 = rag_chain.invoke(question1)\n",
    "\n",
    "print(f\"â“ Question: {question1}\")\n",
    "print(f\"âœ… Answer: {answer1}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94991e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2: The manager's name\n",
    "question2 = \"Who is the manager?\"\n",
    "answer2 = rag_chain.invoke(question2)\n",
    "\n",
    "print(f\"â“ Question: {question2}\")\n",
    "print(f\"âœ… Answer: {answer2}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa037721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3: Something NOT in the document\n",
    "question3 = \"What is the company's revenue?\"\n",
    "answer3 = rag_chain.invoke(question3)\n",
    "\n",
    "print(f\"â“ Question: {question3}\")\n",
    "print(f\"âœ… Answer: {answer3}\")\n",
    "print()\n",
    "print(\"ğŸ’¡ The model correctly says it doesn't have that information!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6e4d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 4: More complex question\n",
    "question4 = \"What are both the primary and backup vault codes?\"\n",
    "answer4 = rag_chain.invoke(question4)\n",
    "\n",
    "print(f\"â“ Question: {question4}\")\n",
    "print(f\"âœ… Answer: {answer4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dcd08b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”¬ Bonus: See the Full RAG Process\n",
    "\n",
    "Let's create a verbose version that shows each step of the RAG process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a677f98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_with_details(question: str):\n",
    "    \"\"\"\n",
    "    Ask a question and show the full RAG process.\n",
    "    \"\"\"\n",
    "    print(\"ğŸ” FULL RAG PROCESS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Show the question\n",
    "    print(f\"\\nğŸ“ QUESTION: {question}\")\n",
    "    \n",
    "    # Step 2: Retrieve relevant chunks\n",
    "    print(\"\\nğŸ¯ STEP 1: RETRIEVAL\")\n",
    "    print(\"-\" * 40)\n",
    "    retrieved = retriever.invoke(question)\n",
    "    for i, doc in enumerate(retrieved, 1):\n",
    "        print(f\"   Chunk {i}: '{doc.page_content[:60]}...'\")\n",
    "    \n",
    "    # Step 3: Show formatted context\n",
    "    context = format_docs(retrieved)\n",
    "    print(\"\\nğŸ“„ STEP 2: FORMATTED CONTEXT\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"   {context[:200]}...\")\n",
    "    \n",
    "    # Step 4: Generate answer\n",
    "    print(\"\\nğŸ¤– STEP 3: LLM GENERATION\")\n",
    "    print(\"-\" * 40)\n",
    "    answer = rag_chain.invoke(question)\n",
    "    print(f\"   {answer}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    return answer\n",
    "\n",
    "# Test it!\n",
    "ask_with_details(\"What is the backup code and when can I use it?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a598e3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ Summary\n",
    "\n",
    "### The RAG Pipeline\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   LOAD      â”‚ â†’  â”‚   SPLIT     â”‚ â†’  â”‚   EMBED     â”‚ â†’  â”‚   STORE     â”‚\n",
    "â”‚ Documents   â”‚    â”‚ Into chunks â”‚    â”‚ Textâ†’Vector â”‚    â”‚ Vector DB   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                                               â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   ANSWER    â”‚ â†  â”‚  GENERATE   â”‚ â†  â”‚  RETRIEVE   â”‚ â†  â”‚  QUESTION   â”‚\n",
    "â”‚  To user    â”‚    â”‚ With LLM    â”‚    â”‚ Similar     â”‚    â”‚ From user   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "âœ… **Embeddings**\n",
    "- Convert text to vectors (lists of numbers)\n",
    "- Similar meanings â†’ Similar vectors\n",
    "- Enables semantic search (by meaning, not keywords)\n",
    "\n",
    "âœ… **Chunking**\n",
    "- Break documents into smaller pieces\n",
    "- Overlap preserves context at boundaries\n",
    "- Smaller chunks = more precise retrieval\n",
    "\n",
    "âœ… **Vector Store (Chroma)**\n",
    "- Stores chunks + their embeddings\n",
    "- Enables fast similarity search\n",
    "- Returns most relevant chunks for a query\n",
    "\n",
    "âœ… **RAG Chain (LCEL)**\n",
    "- Retriever finds context\n",
    "- Prompt combines context + question\n",
    "- LLM generates grounded answer\n",
    "\n",
    "### Modern Imports (2025)\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸš€ Next Steps\n",
    "\n",
    "Now you understand RAG! In the Projects module, you'll build complete applications like:\n",
    "- Chat with your PDFs\n",
    "- Q&A over your codebase\n",
    "- Custom knowledge assistants\n",
    "\n",
    "Happy building! ğŸ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128adb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# CLEANUP (Optional)\n",
    "# ===========================================\n",
    "\n",
    "# If you want to clean up the in-memory vector store\n",
    "# vectorstore.delete_collection()\n",
    "# print(\"ğŸ§¹ Vector store cleaned up!\")\n",
    "\n",
    "print(\"\\nğŸ‰ Congratulations! You've completed the RAG Basics module!\")\n",
    "print(\"You now know how to:\")\n",
    "print(\"   âœ… Load documents\")\n",
    "print(\"   âœ… Split them into chunks\")\n",
    "print(\"   âœ… Create embeddings\")\n",
    "print(\"   âœ… Store in a vector database\")\n",
    "print(\"   âœ… Retrieve relevant context\")\n",
    "print(\"   âœ… Generate answers with RAG\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
